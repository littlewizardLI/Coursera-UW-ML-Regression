
## Machine_Learning-Regression
**2/4 in Machine_Learning-Specialization————A Case Study Approach**

<br /> <br />
---
### Course-Content 学习内容
--
* ** Linear Regression线性回归**

在本模块中，描述了回归任务，然后将这些概念专门用于简单的线性回归。 学习如何制定一个简单的回归模型，并使用封闭式解决方案以及称为梯度下降的迭代优化算法将模型拟合到数据中。 基于此拟合函数，您将解释估计的模型参数和形式预测。 您分析适合离岸观察的敏感度。

<br /> <br />

* **Multiple Regression多项式回归**

使用数据的多个特征来形成预测的“多回归”。
The next step in moving beyond simple linear regression is to consider "multiple regression" where multiple features of the data are used to form predictions.

 <br /> <br />

* **Assessing Performance评估指标**

这个模块是关于模型选择和评估的重要课题

This module is all about these important topics of model selection and assessment     
 <br /> <br />

* **Ridge Regression岭回归**

这是一个非常简单但非常有效的技术，可以自动应对这个问题。 这种方法称为“脊回归”。 您从一个复杂的模型开始，但现在适应模型不仅融合了训练数据的适合度，而且还将一种将解决方案偏离过度配备的功能的术语。

 a very simple, but extremely effective technique for automatically coping with this issue. This method is called "ridge regression". You start out with a complex model, but now fit the model in a manner that not only incorporates a measure of fit to the training data, but also a term that biases the solution away from overfitted functions.     
 <br /> <br />

* **Feature Selection & Lasso特征选择和Lasso技巧**

基本的机器学习任务是在一组要素中选择在模型中的特征。 分析穷举搜索和贪婪算法。 转而使用Lasso回归，它以类似于脊回归的方式隐含地执行特征选择：一个复杂的模型是基于对训练数据的拟合度量
A fundamental machine learning task is to select amongst a set of features to include in a model.analyze both exhaustive search and greedy algorithms. Then, instead of an explicit enumeration, we turn to Lasso regression, which implicitly performs feature selection in a manner akin to ridge regression: A complex model is fit based on a measure of fit to the training data plus a measure of overfitting different than that used in ridg
 <br /> <br />
 
 * **Nearest Neighbors & Kernel Regressio最邻近和核回归**

非参数方法的简单直观的例子，最近邻回归：查询点的预测是基于训练集中最相关的观察结果的输出。 这种方法非常简单，但可以提供出色的预测，特别是对于大型数据集。内核回归使用数据集中的所有观察值，但是这些观察值对预测值的影响是通过与查询点的相似度进行加权的
nearest neighbor regression: The prediction for a query point is based on the outputs of the most related observations in the training set. This approach is extremely simple, but can provide excellent predictions, especially for large dataset。Kernel regression uses all observations in the dataset, but the impact of these observations on the predicted value is weighted by their similarity to the query point.
 <br /> <br />

* For more information：go https://www.coursera.org/learn/ml-foundations
 <br />

## Author
[MingJun Li](https://github.com/littlewizardLI)

## License
[MIT license](https://github.com/littlewizardLI/LICENSE)

